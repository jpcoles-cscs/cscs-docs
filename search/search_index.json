{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["trimmer"]},"docs":[{"location":"","title":"CSCS Documentation","text":"<p>Welcome to the techincal documentation for Alps.</p> <ul> <li> <p> Platforms</p> <p>Once you have a project at CSCS, start here to find your platform:</p> <p> Platforms overview</p> <p>Go straight to the documentation for the platform that hosts your project:</p> <p> HPC Platform</p> <p> Machine Learning Platform</p> <p> Climate and weather Platform</p> </li> <li> <p> Alps</p> <p>Learn more about the Alps research infrastructure</p> <p> Alps Overview</p> <p>Get detailed information about the main components of the infrastructre</p> <p> Alps Hardware</p> <p> Alps Network</p> <p> Alps Storage</p> </li> <li> <p> Getting Access</p> <p>A project is required to access resources on Alps</p> <p> Applying for an Alps project</p> <p>CSCS uses multifactor authentification for secure connection to our services and systems</p> <p> Setting up MFA</p> <p> Logging into vClusters</p> <p>Manage your project and team in the user management portal</p> <p> User management portal</p> </li> </ul>"},{"location":"#get-in-touch","title":"Get in Touch","text":"<p>If you can't find the information that you need in the documentation, help is available.</p> <ul> <li> <p> Get Help</p> <p>Contact the CSCS Service Desk for help.</p> <p> Service Desk</p> </li> <li> <p> Chat</p> <p>Discuss Alps with other users and CSCS staff on Slack.</p> <p> CSCS User Slack</p> </li> </ul>"},{"location":"#tools-and-services","title":"Tools and Services","text":"<p>Todo</p> <p>Provide some links to the \"how\" documentation here.</p> <ul> <li> <p> Tools</p> <p>CSCS provides tools and software on Alps.</p> <p> Tools Overview</p> </li> <li> <p> Build and Install Software</p> <p>Guides on how to build and install software from source using uenv and containers</p> <p> Building and Installing Software</p> </li> </ul>"},{"location":"access/","title":"Access and Accounting","text":"<p>Users at CSCS typically have one account that can be used to access all services at CSCS.</p> <ul> <li> <p> Multi Factor Authetification (MFA)</p> <p>A guide to setting up and using MFA.</p> <p> Setting up and use MFA</p> </li> <li> <p> Getting Access</p> <p>A project is required to get access to resources on Alps. Instructions on how to submit a project proposal is available on the main CSCS web site.</p> <p> Applying for an Alps project</p> </li> </ul> <p>There are different ways to authenticate your identity in order to access services at CSCS, using a password set by the user. Currently users can be authenticated with:</p> <ul> <li>Classic ssh with CSCS username/password, and also with ssh-keys</li> <li>CSCS username/password on different web services. Sessions can be independent from each other so, signing into one service does not necessarily sign the user into another one</li> <li>CSCS username/password on a Single Sign-On gate, which once the user is authenticated, can move between services (connected to this gate) without signing in again</li> <li>Username/password from an external institution, provided that his/her CSCS account has been \"linked\" to that external identity beforehand and the service uses the Single Sign-On gate</li> </ul>"},{"location":"access/#single-sign-on","title":"Single Sign-On","text":"<p>Most services at CSCS are connected to the CSCS Single Sign-On gate. This gives users the comfort of not having to sign in multiple times in each individual service connected to this gate and increases security. Furthermore, the Single Sign-On gate allow users to recover their forgotten passwords and authenticate using a third-party account. The login page looks like</p>"},{"location":"access/ump/","title":"User Management Portal","text":"<p>Todo</p> <p>copy over docs from confluence</p> <p>15 minute job</p>"},{"location":"access/waldur/","title":"Waldur","text":""},{"location":"access/waldur/#user-management-with-waldur","title":"User Management with Waldur","text":"<p>CSCS Account Managers, PIs and deputy PIs can invite users to the respective projects following the below steps on CSCS's new project management portal.</p> <p>Info</p> <p>The new user project management portal is currently only used by the Machine Learning Platform All other platforms use the user management portal (UMP)</p>"},{"location":"access/waldur/#log-in-to-the-portal","title":"log in to the portal","text":"<p>Navigate to the site project management portal portal.cscs.ch.</p>"},{"location":"access/waldur/#select-the-organisation","title":"Select the Organisation","text":"<p>After login to the portal, choose the corresponding organization in which the project was created.</p> <p>Todo</p> <p>screenshot</p> <p>In this example, The project was hosted by the CSCS organization, and say the project name is <code>csstaff_n</code>, From the organization dashboard navigate to Projects and click on <code>csstaff_n</code> Project</p> <p>Todo</p> <p>screenshot</p>"},{"location":"access/waldur/#invite-users","title":"Invite users","text":"<p>From the project dashboard, navigate to Team -&gt; Invitations</p> <p>Todo</p> <p>screenshot</p> <p>Info</p> <p>Using both the web interface and bulk invitation, the following roles can be assigned in the tool:</p> <ul> <li>Project administrator: PI</li> <li>Project manager:  deputy PI</li> <li>Project member: team member</li> </ul> invite individual usersbulk invite <p>To invite a user, click on the \"Invite Users\" button on the right hand side of the tab.</p> <p>Todo</p> <p>screenshot</p> <p>Todo</p> <p>screenshot</p> <p>It is also possible to bulk invite users by preparing a CSV file and uploading it in this step.</p> <pre><code>Email,Role,Project\nCragAlvarado@example.com,Project member,prj02\nAndrease@example.com,Project member,prj02\nJoannWaters@example.com,Project administrator,prj02\nDonnaSchwartz@example.com,Project manager,prj02\n</code></pre> <p>Note</p> <p>An email will be sent to the invited user:</p> <ul> <li>users who already have CSCS accounts should click on the link in the email they received, and authenticate against CSCS KeyCloak with username, password, and OTP to accept the invitation.</li> <li>new users should follow the procedure to create a CSCS account.</li> </ul>"},{"location":"access/mfa/","title":"Index","text":""},{"location":"access/mfa/#multi-factor-authentification","title":"Multi Factor Authentification","text":"<p>Todo</p> <p>These docs have been mostly cut and paste, with some small changes.</p> <p>I did some refactoring to make them a bit easier to read, and more logical for linking to other docs.</p> <p>But they should be more user friendly, because they will often be the first docs that users interact with as they acccess CSCS.</p> <p>To access CSCS services and systems users are required to authenticate using multi-factor authentication (MFA). MFA is implemented as a two-factor authentication, where one factor is the login and password pair (\"the thing you know\") and the other factor is the device which generates one-time passwords (OTPs, \"the thing you have\"). In this way security is significantly improved compared to single-factor (password only) authentication.</p> <p>The MFA workflow uses a time-based one-time password (OTP) to verify identity. An OTP is a six-digit number which changes every 30 seconds. OTPs are generated using a tool installed on a device other than the one used to access CSCS services and infrastructure. We recommend to use a smartphone with an application such as Google Authenticator to obtain the OTPs.</p> <ul> <li> <p> Getting Started</p> <p>Get the authenticator app set up on your phone.</p> </li> <li> <p> Log in to a cluster</p> <p>Use SSH to log into a cluster.</p> </li> <li> <p> Access a CSCS web app</p> <p>MFA is required to log into CSCS web services.</p> </li> </ul> <p></p>"},{"location":"access/mfa/#getting-started","title":"Getting Started","text":"<p>When you first log in to any of the CSCS web applications such as UMP, Jupyter, etc., you will be asked to register your device.</p> <p>Firstly, you will be asked to provide a code that you received by email. After this validation step, you will need to scan a QR code with your mobile phone using an application such as Google Authenticator. Lastly, you will need to enter the OTP from the authenticator application to complete the registration of your device. From then on, two-factor authrentication will be required to access CSCS services and systems. A more detailed explanation of the registration process is provided in the next section.</p> <p>Warning</p> <p>It is not possible to log in to CSCS systems using SSH without registering a device and creating certified SSH keys. See below for details on generating certified SSH keys.</p>"},{"location":"access/mfa/#authenticator-application","title":"Authenticator Application","text":"<p>CSCS supports authenticators that follow an open standard called TOTP. The recommended way to access such an authenticator is to install an application on your mobile phone. Google Authenticator and FreeOTP have been tested successfully; however, if you are using a different mobile application for OTPs, feel free to continue using it - given it supports the TOTP standard.</p> <p>You can download Google Authenticator for your phone:</p> <ul> <li> Android: on the Google Play Store.</li> <li> iOS: on the Apple Store.</li> </ul> <p></p>"},{"location":"access/mfa/#configure-the-authenticator","title":"Configure the Authenticator","text":"<p>Before starting, ensure that the following pre-requisites are satisfied</p> <ol> <li>You have an invitation email from CSCS for MFA enrollment<ul> <li>a notification email will be sent atleast one week before we sent the invitation email.</li> </ul> </li> <li>You have installed an OTP Authenticator app on your mobile device (see above).</li> </ol> <p>Note</p> <p>If you try access any of our web applications without setting up MFA, you will be redirected to enroll for MFA.</p> <p>Warning</p> <p>If you try to SSH to CSCS systems without setting up MFA, you will be prompted with permission denied error, for example: <pre><code>&gt; ssh ela.cscs.ch\nbobsmith@ela.cscs.ch: Permission denied (publickey).\nConnection closed by UNKNOWN port 65535\n</code></pre></p> <p>Steps:</p> <ol> <li>Access any of the CSCS Web applications such as <code>account.cscs.ch</code>, Jupyter, etc., on a new browser session which will redirects you to the CSCS login page.</li> <li>Log in with your username and password.</li> <li>You will be asked to key in a code which CSCS Authentication system sent to you by email.    After successfaul validation of the code you will be redirected to the next page which present a QR code.</li> <li>Scan the QR code with the authenticator app that was installed on your mobile device.    After scanning the QR code the authenticator app will start generating a new 6 digit OTP every 60 seconds.</li> <li>To complete the OTP registration process, please enter the 6 digit OTP from the authenticator app at the bottom of the the same QR code page. Optionally, you can input your device name where you imported the OTP seed by scanning the QR code</li> <li>On successful registration you will be logged into the CSCS web application that you accessed in step-1</li> </ol> <p>Todo</p> <p>do we need the images from KB?</p>"},{"location":"access/mfa/#resetting-the-authenticator","title":"Resetting the Authenticator","text":"<p>In case users lose access to their mobile device/Authenticator OTP, users can reset their OTP by following the below self-service process.</p> <ol> <li>Access any CSCS web application like: account.cscs.ch which redirects you to the CSCS Login page. </li> <li>From the login screen, click the \"Reset OTP\" link below the \"LOG IN\" button</li> <li>Enter your username and password.</li> <li>On successful validation of user credentials, users will receive an email with a reset credentials link like the one below, click on the link in the email</li> <li>The steps are the same as for the first time you configured the authenticator.</li> </ol> <p>Warning</p> <p>When replacing your smartphone remember to sync the authenticator app before resetting the old smartphone. Otherwise, you will have to follow this process.</p> <p></p>"},{"location":"access/mfa/#using-mfa-to-acccess-web-based-services","title":"Using MFA to acccess web-based services","text":"<p>After having completed the setup of your authenticator, you will be asked to enter your login/password and the OTP to access all web-based services.</p> <ol> <li>Enter username and password.<ul> <li>if the wrong username / password combination is entered, you will see an error. </li> </ul> </li> <li>Then you will be prompted to enter the 6-digit code obtained from your device.     </li> </ol> <p></p>"},{"location":"access/mfa/#using-mfa-with-ssh","title":"Using MFA with SSH","text":"<p>It is not possible to authenticate with a username/password and user-created SSH keys: it is necessary to use a certified SSH key created by a CSCS SSHService.</p> <p>Note</p> <p>Keys are valid for 24 hours, after which a new key must be generated.</p> <p>Warning</p> <p>The number of certified SSH keys is limited to five per day. Once you have reached this number you will not be able to generate new keys until at least one of these key expires or keys are revoked.</p>"},{"location":"access/mfa/#step-1-generate-ssh-keys","title":"Step 1: generate SSH keys","text":"<p>There are two methods for generating SSH keys using the SSHService, the SSHService web app or by using a command-line script.</p>"},{"location":"access/mfa/#getting-keys-via-the-command-line","title":"Getting keys via the command line","text":"<p>On Linux and MacOS, the SSH keys can be generated and automatically installed using a command-line script. This script is provided in pure Bash and in Python. Python 3 is required together with packages listed in <code>requirements.txt</code> provided with the scripts.</p> <p>Note</p> <p>We recommend to using a virtual environment for Python.</p> <p>If this is the first time, download the ssh service from CSCS GitHub:</p> <pre><code>git clone https://github.com/eth-cscs/sshservice-cli\ncd sshservice-cli\n</code></pre> <p>The next step is to use either the bash or python scripts:</p> bashpython <p>Run the bash script in the <code>sshservice-cli</code> path:</p> <pre><code>./cscs-keygen.sh\n</code></pre> <p>The first time you use the script, you can set up a python virtual environment with the dependencies installed:</p> <pre><code>python3 -m venv mfa\nsource mfa/bin/activate\npip install -r requirements.txt\n</code></pre> <p>Therafter, activate the venv before using the script:</p> <pre><code>source mfa/bin/activate\npython cscs-keygen.py\n</code></pre> <p>For both approaches, follow the on screen instructions that require you to enter your username, password and the six-digit OTP from the authentifactor app on your phone. The script generates the key pair (<code>cscs-key</code> and <code>cscs-key-cert.pub</code>) in your <code>~/.ssh</code> path:</p> <pre><code>&gt; ls ~/.ssh/cscs-key*\n/home/bobsmith/.ssh/cscs-key  /home/bobsmith/.ssh/cscs-key-cert.pub\n</code></pre>"},{"location":"access/mfa/#getting-keys-via-the-web-app","title":"Getting keys via the web app","text":"<p>Access the SSHService web application by accessing the URL, sshservice.cscs.ch.</p> <ol> <li>Sign in with username, password and OTP</li> <li>Select \"Signed key\" on the left tab and click on \"Get a signed key\"</li> <li>On the next page a key pair is generated and ready to be downloaded. Download or copy/paste both keys.</li> </ol> <p>Once generated, the keys need to be copied from where your browser downloaded them to your <code>~/.ssh</code> path, for example: <pre><code>mv /download/location/cscs-key-cert.pub ~/.ssh/cscs-key-cert.pub\nmv /download/location/cscs-key ~/.ssh/cscs-key\nchmod 0600 ~/.ssh/cscs-key\n</code></pre></p>"},{"location":"access/mfa/#step-2-log-in-with-the-generated-keys","title":"Step 2: log in with the generated keys","text":"<p>Set up a passphrase on the private key: <pre><code>ssh-keygen -f ~/.ssh/cscs-key -p\n</code></pre></p> <p>Add the key to the SSH agent: <pre><code>ssh-add -t 1d ~/.ssh/cscs-key\n</code></pre></p> Could not open a connection to your authentification agent <p>If you see this error message, the ssh agent is not running. You can start it with the following command: <pre><code>eval $(ssh-agent)\n</code></pre></p> <p>Once the key has been configured, you can log in to CSCS' login system Ela: <pre><code># log in to ela.cscs.ch\n&gt; ssh -A cscs_username@ela.cscs.ch\n\n# then jump to a cluster\n&gt; ssh clariden\n</code></pre></p>"},{"location":"access/mfa/#frequently-encountered-issues","title":"Frequently encountered issues","text":"too many authentification failures <p>You may have too many keys in your ssh agent. Remove the unused keys from the agent or flush them all with the following command: <pre><code>ssh-add -D\n</code></pre></p> Permission denied <p>This might indicate that they key has expired.</p> Could not open a connection to your authentication agent <p>If you see this error when adding keys to the ssh-agent, please make sure the agent is up, and if not bring up the agent using the following command: <pre><code>eval $(ssh-agent)\n</code></pre></p> <p>Todo</p> <p>We need a \"best practices\"/\"hints and tips\" section for setthing up <code>~/.ssh/config</code></p>"},{"location":"access/mfa/windows/","title":"Using MFA with Windows","text":"<p>Todo</p> <p>copy over the MFA with windows docs from Confluence.</p> <p>Todo</p> <p>the windows docs are pretty terrible, and they just repeat the same steps as the main docs, but in WSL or MobaXterm. Can't we just have a helpful expandable tab in the main docs that says \"use WSL on windows\"?</p> <p>The workarounds for MobaXterm issues etc. can be added to the \"common problems\" list</p>"},{"location":"alps/","title":"Alps Infrastructure","text":"<p>Alps is a general-purpose compute and data Research Infrastructure (RI) open to the broad community of researchers in Switzerland and the rest of the world. Alps will provide a high impact, challenging and innovative RI that will allow Switzerland to advance science and impact society.</p> <p>Alps enables the creation of versatile clusters (vClusters) that can be tailored to the specific needs of users while maintaining confidentiality. For example, a vCluster will be dedicated to MeteoSwiss\u2019 numerical weather forecasts, another one to the User Lab and another one to Machine Learning and Artificial Intelligence.</p> <ul> <li> <p> Hardware</p> <p>Learn about the node types and networking infrastructure in Alps.</p> <p> Alps Hardware</p> </li> <li> <p> Network</p> <p>Learn about the Slingshot 11 network on Alps.</p> <p> Alps Network</p> </li> <li> <p> Storage</p> <p>Learn about the file systems attached to Alps.</p> <p> Alps Storage</p> </li> <li> <p> vClusters</p> <p>The resources on Alps are partitioned and configured into versatile software defined clusters (vClusters).</p> <p> Alps vClusters</p> </li> <li> <p> Tenants</p> <p>Alps is a multi-tenant system.</p> <p> Alps Tenants</p> </li> </ul>"},{"location":"alps/hardware/","title":"Alps Hardware","text":"<p>Alps is a HPE Cray EX3000 system, a liquid cooled blade-based, high-density system.</p> <p>Todo</p> <p>this is a skeleton - all of the details need to be filled in</p>"},{"location":"alps/hardware/#alps-cabinets","title":"Alps Cabinets","text":"<p>The basic building block of the system is a liquid-cooled cabinet. A single cabinet can accommodate up to 64 compute blade slots within 8 compute chassis. The cabinet is not configured with any cooling fans. All cooling needs for the cabinet are provided by direct liquid cooling and the CDU. This approach to cooling provides greater efficiency for the rack-level cooling, decreases power costs associated with cooling (no blowers) and utilizes a single water source per CDU One cabinet supports the following:</p> <ul> <li>8 compute chassis</li> <li>4 power shelves with a maximum of 6 rectifiers per shelf- 24 total 12.5 or 15kW rectifiers per cabinet</li> <li>4 PDUs (1 per power shelf)</li> <li>3 power input whips (3-phase)</li> <li>Maximum of 64 quad-blade compute blades</li> <li>Maximum of 64 Slingshot switch blades</li> </ul>"},{"location":"alps/hardware/#alps-blades","title":"Alps Blades","text":"<p>Alps was installed in phases, starting with the installation of 1024 AMD Rome dual socket CPU nodes in 2020, through to the main installation of 2,688 Grace-Hopper nodes in 2024.</p> <p>There are currently four node types in Alps, with another becoming available in 2025:</p> type blades nodes CPU sockets GPU devices NVIDIA GH200 1344 2688 10,752 10,752 AMD Rome 256 1024 2,048 -- NVIDIA A100 72 144 144 576 AMD MI250x 12 24 24 96 AMD MI300A 64 128 512 512"},{"location":"alps/hardware/#nvidia-gh200-gpu-nodes","title":"NVIDIA GH200 GPU Nodes","text":"<p>Perry Peak</p>"},{"location":"alps/hardware/#amd-rome-cpu-nodes","title":"AMD Rome CPU Nodes","text":"<p>EX425</p>"},{"location":"alps/hardware/#nvidia-a100-gpu-nodes","title":"NVIDIA A100 GPU Nodes","text":"<p>Grizzly Peak</p>"},{"location":"alps/hardware/#amd-mi250x-gpu-nodes","title":"AMD MI250x GPU Nodes","text":"<p>Bard Peak</p>"},{"location":"alps/hardware/#amd-mi300a-gpu-nodes","title":"AMD MI300A GPU Nodes","text":"<p>Parry Peak</p> <p>coming soon</p> <p>H1 2025</p>"},{"location":"alps/network/","title":"Alps High Speed Network","text":"<p>Todo</p> <p>information about the network.</p> <ul> <li>Details about SlingShot 11.<ul> <li>how many NICS per node</li> <li>raw feeds and speeds</li> </ul> </li> <li>Some OSU benchmark results.</li> <li>GPU-aware communication</li> <li>slingshot is not infiniband - there is no NVSwitch</li> </ul>"},{"location":"alps/storage/","title":"Alps Storage","text":"<p>Todo</p> <p>Document the main storage hardware attached to Alps:</p> <ul> <li>capstor</li> <li>iopstor</li> <li>vast</li> </ul> <p>The focus of these docs would be the basic details.</p> <p>The mounts, and how they are used for SCRATCH, STORE, PROJECT, HOME would be in the storage docs</p>"},{"location":"alps/tenants/","title":"Alps Tenants","text":"<p>Todo</p> <p>This page answeres the question \"what is a tenant\"</p> <ul> <li>why/how is Alps multi tenant</li> <li>who are the tenants</li> </ul>"},{"location":"alps/vclusters/","title":"Alps vClusters","text":"<p>Todo</p> <p>this page answers the question \"what is a vCluster\"?</p> <ul> <li>What is a vCluster?</li> <li>Examples of vClusters</li> </ul> <p>We don't document individual vClusters here - these are documented under each platform.</p>"},{"location":"build-install/","title":"Building and Installing Software","text":"<p>CSCS provides commonly used software and tools on Alps, however many use cases will require first installing software on a system before you can start working.</p> <p>Modern HPC applications and software stacks are often very complicated, and there is no one-size-fits-all method for building and installning them.</p>"},{"location":"build-install/#programming-environments","title":"Programming environments","text":"<ul> <li> uenv \u2013 uenv provide isolated software environments for applications and developers.</li> <li> CPE \u2013 the Cray Programming Environment is provided as-is on Alps.</li> </ul>"},{"location":"build-install/#python","title":"Python","text":"<p>There are multiple ways to install Python software.</p> <ul> <li> pip \u2013 create a virtual environment using python in a uenv</li> </ul>"},{"location":"build-install/#containers","title":"Containers","text":"<p>CSCS provides tools for building software in containers</p>"},{"location":"build-install/containers/","title":"Building container images on Alps","text":"<p>Building OCI container images on Alps vClusters is supported through Podman, an open-source container engine that adheres to OCI standards and supports rootless containers by leveraging Linux user namespaces. Its command-line interface (CLI) closely mirrors Docker\u2019s, providing a consistent and familiar experience for users of established container tools.</p>"},{"location":"build-install/containers/#preliminary-step-configuring-podmans-storage","title":"Preliminary step: configuring Podman's storage","text":"<p>The first step in order to use Podman on Alps is to create a valid Container Storage configuration file at <code>$HOME/.config/containers/storage.conf</code>, according to the following minimal template:</p> <pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/dev/shm/$USER/runroot\"\ngraphroot = \"/dev/shm/$USER/root\"\n</code></pre> <p>Warning</p> <p>In the above configuration, <code>/dev/shm</code> is used to store the container images. <code>/dev/shm</code> is the mount point of a tmpfs filesystem and is compatible with the user namespaces used by Podman. The limitation of this approach  is that container images created during a job allocation are deleted when the job ends. Therefore, the image needs to either be pushed to a container registry or imported by the Container Engine before the job allocation finishes.</p>"},{"location":"build-install/containers/#building-images-with-podman","title":"Building images with Podman","text":"<p>The easiest way to build a container image is to rely on a Containerfile (a more generic name for a container image recipe, but essentially equivalent to Dockerfile):</p> <pre><code># Allocate a compute node and open an interactive terminal on it\nsrun --pty --partition=&lt;partition&gt; bash\n\n# Change to the directory containing the Containerfile/Dockerfile and build the image\npodman build -t &lt;image:tag&gt; .\n</code></pre> <p>In general, <code>podman build</code> follows the Docker options convention.</p>"},{"location":"build-install/containers/#importing-images-in-the-container-engine","title":"Importing images in the Container Engine","text":"<p>An image built using Podman can be easily imported as a squashfs archive in order to be used with our Container Engine solution. It is important to keep in mind that the import has to take place in the same job allocation where the image creation took place, otherwise the image is lost due to the temporary nature of <code>/dev/shm</code>.</p> <p>To import the image:</p> <pre><code>enroot import -x mount -o &lt;image_name.sqsh&gt; podman://&lt;image:tag&gt;\n</code></pre> <p>The resulting <code>&lt;image_name.sqsh&gt;</code> can used directly as an explicitly pulled container image, as documented in Container Engine. An example Environment Definition File (EDF) using the imported image looks as follows:</p> <pre><code>image = \"/&lt;path to image directory&gt;/&lt;image_name.sqsh&gt;\"\nmounts = [\"/capstor/scratch/cscs/&lt;username&gt;:/capstor/scratch/cscs/&lt;username&gt;\"]\nworkdir = \"/capstor/scratch/cscs/&lt;username&gt;\"\n</code></pre>"},{"location":"build-install/containers/#pushing-images-to-a-container-registry","title":"Pushing Images to a Container Registry","text":"<p>In order to push an image to a container registry, you first need to follow three steps:</p> <ol> <li>Use your credential to login to the container registry with podman login.</li> <li>Tag the image according to the name of your container registry and the corresponding repository, using podman tag. This step can be skipped if you already provided the appropriate tag when building the image.</li> <li>Push the image using podman push.</li> </ol> <pre><code># Login to a container registry using username/password interactively\npodman login &lt;registry_url&gt;\n\n# Tag the image accordingly\npodman tag &lt;image:tag&gt; &lt;registry url&gt;/&lt;image:tag&gt;\n\n# Push the image (for docker type registries use the docker:// prefix)\npodman push docker://&lt;registry url&gt;/&lt;image:tag&gt;\n</code></pre> <p>For example, to push an image to the DockerHub container registry, the following steps have to be performed:</p> <pre><code># Login to DockerHub (Podman will ask for your credentials)\npodman login docker.io\n\n# Tag the image based on your username\npodman tag &lt;image:tag&gt; docker.io/&lt;username&gt;/myimage:latest\n\n# Push the image to the repository of your choice\npodman push docker://docker.io/&lt;username&gt;/myimage:latest\n</code></pre>"},{"location":"build-install/cpe/","title":"Cray Programming Environment (CPE)","text":"<p>Warning</p> <p>you don't want to use this</p> <p>Mlp</p> <p>The CPE is not provided on the machine learning platform.</p> <p>Cwp</p> <p>The CPE is not provided on the climage and weather platform</p>"},{"location":"build-install/pip/","title":"Installing Python software with pip","text":"<p>todo</p>"},{"location":"build-install/uenv/","title":"uenv","text":"<p>Uenv are user environments that provide scientific applications, libraries and tools on Alps. This article use them to build software.</p> <p>For more documentation on how to find, download and use uenv in your workflow, see the env tool documentation.</p>"},{"location":"build-install/uenv/#building-software-using-spack","title":"Building software using Spack","text":"<p>Each uenv is tightly coupled with Spack and can be used as an upstream Spack instance, because the sofware in uenv is built with Spack using the Stackinator tool.</p> <p>CSCS provides <code>uenv-spack</code> - a tool that can be used to quickly install software using the software and configuration provided inside a uenv.</p>"},{"location":"build-install/uenv/#installing-uenv-spack","title":"Installing <code>uenv-spack</code>","text":"<pre><code># download the uenv-spack tool\ngit clone https://github.com/eth-cscs/uenv-spack.git\n\n# initialise the tool\n(cd uenv-spack &amp;&amp; ./bootstrap)\n\nexport PATH=$PWD/uenv-spack:$PATH\n</code></pre>"},{"location":"build-install/uenv/#select-the-uenv","title":"Select The uenv","text":"<p>The next step is to choose which uenv to use. The uenv will provide the compilers, cray-mpich, and other libraries and tools.</p> <pre><code>graph TD\n  A[/is there a uenv for the application?\\] --&gt;|yes| B[use that image, e.g. **gromacs**]\n  A --&gt; |no| C[/do I need OpenACC or CUDA Fortran?\\]\n  C --&gt; |no| D[use **prgenv-gnu**]\n  C --&gt; |yes| E[/are you _really_ sure?\\]\n  E --&gt; |yes| F[use **prgenv-nvfortran**]\n  E --&gt; |no| D</code></pre> <p>use <code>prgenv-gnu</code> when in doubt</p> <p>If you don't know where to start, use the latest release of the <code>prgenv-gnu</code> on the system that you are targeting. It provides the latest versions of <code>gcc</code>, <code>cray-mpich</code>, <code>python</code> and commonly used libraries like <code>fftw</code> and <code>boost</code>.</p> <p>On systems that have NVIDIA GPUs (<code>gh200</code> and <code>a100</code> uarch), it also provides the latest version of <code>cuda</code> and <code>nccl</code>, and is configured for GPU-aware MPI communication.</p> <p>To use Spack as an upstream, the uenv has to be started with the <code>spack</code> view:</p> <pre><code>uenv start prgenv-gnu/24.11:v1 --view=spack\n</code></pre> what does the <code>spack</code> view do? <p>The <code>spack</code> view sets environment variables that provide information about the version of Spack that was used to build the uenv, and where the uenv Spack configuration is stored.</p> <p>This information is useful because it is strongly recomended that you use the same version of Spack to build software on top of the uenv.</p> variable example description <code>UENV_SPACK_CONFIG_PATH</code> <code>user-environment/config</code> the path of the upstream spack configuration files. <code>UENV_SPACK_REF</code> <code>releases/v0.23</code> the branch or tag used - this might be empty if a specific commit of Spack was used. <code>UENV_SPACK_URL</code> <code>https://github.com/spack/spack.git</code> The git repository for Spack - nearly always the main spack/spack repository. <code>UENV_SPACK_COMMIT</code> <code>c6d4037758140fe...0cd1547f388ae51</code> The commit of Spack that was used"},{"location":"build-install/uenv/#describing-what-to-build","title":"Describing what to build","text":"<p>Create a build path with a template <code>spack.yaml</code> and <code>repo</code>:</p> <p><pre><code>uenv-spack &lt;build-path&gt; --uarch=gh200\ncd &lt;build-path&gt;\n./build\n</code></pre> Where <code>&lt;build-path&gt;</code> is a path (typically in <code>$SCRATCH</code>, e.g. <code>$SCRATCH/builds/gromacs-24.11</code>).</p> <p><code>uenv-spack</code> creates a directory tree with the following contents:</p> <pre><code>&lt;build-path&gt;\n\u251c\u2500 build        # build the software stack (script)\n\u251c\u2500 spack        # a git clone of the required version of Spack\n\u251c\u2500 config       # spack configurations for the software stack\n\u2502   \u251c\u2500 meta.json # information about the uenv that was used\n\u2502   \u251c\u2500 user\n\u2502   \u2502  \u251c\u2500 config.yaml\n\u2502   \u2502  \u251c\u2500 modules.yaml\n\u2502   \u2502  \u2514\u2500 repos.yaml\n\u2502   \u2514\u2500 system\n\u2502      \u251c\u2500 compilers.yaml\n\u2502      \u251c\u2500 packages.yaml\n\u2502      \u251c\u2500 repos.yaml\n\u2502      \u2514\u2500 upstreams.yaml\n\u2514\u2500 env          # description of the software to build\n    \u251c\u2500 spack.yaml\n    \u2514\u2500 repo\n       \u251c\u2500 repo.yaml\n       \u2514\u2500 packages\n</code></pre> <p>The <code>env</code> path contains a template <code>spack.yaml</code> file, and an empty Spack package repository:</p> <pre><code>env\n\u251c\u2500 spack.yaml\n\u2514\u2500 repo\n   \u251c\u2500 repo.yaml\n   \u2514\u2500 packages\n</code></pre> <p>where the <code>spack.yaml</code> file contains an empty list of specs:</p> <pre><code>    specs: []\n</code></pre> <p>Edit this file to add the specs that you wish to build, for example:</p> <pre><code>    specs: [tree, screen, emacs +treesitter]\n</code></pre> <p>The step of adding a list of specs to the <code>spack.yaml</code> template can be skipped by providing them using the <code>--specs</code> argument to <code>uenv-spack</code>.</p> <p>Create a build path and populate the <code>spack.yaml</code> file with some specs</p> <pre><code>uenv-spack $SCRATCH/install/tools --uarch=gh200 \\\n           --specs=\"tree, screen, emacs +treesitter\"\ncd $SCRATCH/install/tools\n./build\n</code></pre> <p>If you already have a directory with a complete <code>spack.yaml</code> file and custom repo, you can provide it as an argument to <code>uenv-spack</code>:</p> <p>Create a build path and use a pre-configured <code>spack.yaml</code> and <code>repo</code></p> <pre><code>uenv-spack $SCRATCH/install/arbor --uarch=gh200 \\\n           --recipe=&lt;path-to-recipe&gt;\ncd $SCRATCH/install/tools\n./build\n</code></pre> <p>Create a build path and use your own <code>spack.yaml</code></p> <p>TODO this feature has not been implemented yet <pre><code>uenv-spack $SCRATCH/install/arbor --uarch=gh200 \\\n           --recipe=&lt;path-to-spack.yaml&gt;\ncd $SCRATCH/install/tools\n./build\n</code></pre></p>"},{"location":"build-install/uenv/#build-the-software","title":"Build the software","text":"<p>Once specs have been added to <code>spack.yaml</code>, you can build the image using the <code>build</code> script that was generated in <code>&lt;build-dir&gt;</code>:</p> <pre><code>./build\n</code></pre> <p>This process will take a few minutes at least, because the version of Spack that was downloaded needs to</p> <ul> <li>bootstrap;</li> <li>concretise the environment;</li> <li>build all of the packages.</li> </ul> <p>The duration of the build depends on the specs: some specs may require a long time to build, or require installing many dependencies.</p> <p>The build step generates multiple outputs:</p>"},{"location":"build-install/uenv/#installed-packages","title":"installed packages","text":"<p>The packages built by Spack are installed in <code>&lt;build-dir&gt;/store</code>.</p>"},{"location":"build-install/uenv/#spack-view","title":"Spack view","text":"<p>A Spack view is generated in <code>&lt;build-dir&gt;/view</code>.</p>"},{"location":"build-install/uenv/#modules","title":"modules","text":"<p>Module files are generated in the <code>module</code> sub-directory of the <code>&lt;build-path&gt;</code></p> <p>To use them, add them to the module environment</p> <pre><code># make the modules available\nmodule use &lt;build-dir&gt;/modules\n\n# they should no be visible, check them:\nmodule avail\n</code></pre> <p>Note</p> <p>The generation of modules can be customised by editing the <code>&lt;build-dir&gt;/config/user/modules.yaml</code> file before running <code>build</code>. See the Spack modules documentation.</p>"},{"location":"build-install/uenv/#use-the-software","title":"Use the software","text":"<p>Warning</p> <p>This step is not fully covered by the tool/workflow yet</p> <p>Warning</p> <p>The uenv that was used to configure and build must always be loaded when using the software stack.</p> <ul> <li>[option] load modules</li> <li>[option] activate the view</li> <li>[option] <code>source &lt;build-dir&gt;/spack/share/spack/setup-env.sh</code> then <code>spack find</code>, <code>spack load</code>, <code>spack -e env ...</code> etc.</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Todo</p> <p>Much of the content that is in our current FAQ could go into proper documentation.</p>"},{"location":"platforms/","title":"Platforms on Alps","text":"<p>Todo</p> <p>A high level paragraph that describes what platforms are</p> <ul> <li> <p> Machine Learning Platform</p> <p>The Machine Learning Platform (MLp) hosts ML and AI researchers</p> <p> MLp</p> </li> <li> <p> HPC Platform</p> <p>Todo</p> <p> HPCp</p> </li> <li> <p> Climate and Weather Platform</p> <p>Todo</p> <p> CWp</p> </li> </ul>"},{"location":"platforms/cwp/","title":"Index","text":""},{"location":"platforms/cwp/#climate-and-weather-platform","title":"Climate and Weather Platform","text":"<p>Todo</p> <p>follow the template of the MLp</p>"},{"location":"platforms/hpcp/","title":"Index","text":""},{"location":"platforms/hpcp/#hpc-platform","title":"HPC Platform","text":"<p>Todo</p> <p>follow the template of the MLp</p>"},{"location":"platforms/mlp/","title":"Index","text":""},{"location":"platforms/mlp/#machine-learning-platform","title":"Machine Learning Platform","text":"<p>Todo</p> <p>A description of the MLP</p> <ul> <li>who are the users (help answer the question \"is this the platform that I am on\")</li> <li>who are the partners (SwissAI, etc)</li> <li>how to get apply to access MLp (if that is a thing)</li> </ul>"},{"location":"platforms/mlp/#getting-started","title":"Getting Started","text":""},{"location":"platforms/mlp/#getting-access","title":"Getting access","text":"<p>Project administrators (PIs and deputy PIs) of projects on the MLp can to invite users to join their project, before they can use the project's resources on Alps. This is performed using the project managemant tool</p> <p>Once invited to a project, you will receive an email, which you can need to create an account and configure multi-factor authentification (MFA).</p>"},{"location":"platforms/mlp/#vclusters","title":"vClusters","text":"<p>The main cluster provided by the MLp is Clariden, a large Grace-Hopper GPU system on Alps.</p> <p>Todo</p> <p>introduction paragraph and cards that link to Clariden and Bristen</p>"},{"location":"platforms/mlp/#guides-and-tutorials","title":"Guides and Tutorials","text":"<p>Todo</p> <p>links to tutorials and guides for ML workflows</p>"},{"location":"storage/","title":"Storage","text":"<ul> <li> <p> File Systems</p> <p>Learn about the filesystems on Alps</p> <p> File Systems</p> </li> <li> <p> Data Transfer</p> <p>Moving data into and out of CSCS, and between CSCS systems.</p> <p> Data Transfer</p> </li> <li> <p> Long Term Storage</p> <p>The Long Term Storage (LTS) service enables CSCS users to preserve their scientific data and ensures that it can be publicly accessed through a persistent identifier.</p> <p> LTS</p> </li> <li> <p> Object Storage</p> <p>CSCS offers a public cloud object storage service, based on the Ceph Object Gateway.</p> <p> Object Storage</p> </li> </ul>"},{"location":"storage/filesystems/","title":"File Systems","text":"<p>CSCS supports different file systems, whose specifications are summarized in the table below:</p> <code>$SCRATCH</code> <code>$PROJECT</code> <code>$STORE</code> <code>$HOME</code> <code>$PROJECT_ID</code> <code>$SCRATCH_ID</code> <code>$STORE_ID</code> Access Speed Fast Fast Fast Slow Fast Medium Slow Capacity 8.8 PB 91 PB 1.9 PB 160 TB 91 PB 6.0 PB 7.6 PB Data Backup None None None 90 days 90 days 90 days 90 days Expiration 30 days 30 days 30 days Account closure End of the project/contract End of the project End of the contract Quota Soft (1M files) Soft (150 TB and 1M files) None 50GB/user and 500k files 150 TB and 1M files Maximum 50k files/TB Maximum 50k files/TB Type Lustre Lustre GPFS GPFS Lustre GPFS GPFS <p>To check your usage, please type the command <code>quota</code> on the front end Ela.</p> <p>Please build big software projects not fitting <code>$HOME</code> on <code>$PROJECT</code> instead. Since you should not run jobs from <code>$HOME</code> or <code>$PROJECT</code>, please copy the executables, libraries and data sets needed to run your simulations to <code>$SCRATCH</code> with the Slurm transfer queue.</p> <p>Users can also write temporary builds on <code>/dev/shm</code>, a filesystem using virtual memory rather than a persistent storage device: please note that files older than 24 hours will be deleted automatically.</p>"},{"location":"storage/filesystems/#scratch","title":"Scratch","text":"<p>The scratch file system is designed for performance rather than reliability, as a fast workspace for temporary storage. All CSCS systems provide a scratch personal folder for users that can be accessed through the environment variable <code>$SCRATCH</code>.</p> <p>Alps provides a Lustre scratch file system mounted on <code>/capstor/scratch/cscs</code>, while other clusters share the GPFS scratch file system under <code>/scratch/shared</code>.</p>"},{"location":"storage/filesystems/#soft-quota","title":"Soft Quota","text":"<p>No strict quotas are enforced on scratch, but the scratch file system on Alps (<code>/capstor/scratch/cscs</code>) has a soft quota on both disk occupancy and inodes (files and folders), with a grace period to allow data transfer. Note that when the grace time expires, the soft quotas will become hard limits if you are over quota, therefore you won\u2019t be able to write any longer on your personal scratch folder.</p> <p>Alps (Eiger) users need to check their disk space and inodes usage with the command quota, that is available on the front end Ela and on Eiger User Access Nodes (UANs) as well. Currently the soft quotas are 150 TB disk space and 1 million inodes on Alps scratch file system, with a grace time of two weeks.</p>"},{"location":"storage/filesystems/#cleaning-policy","title":"Cleaning Policy","text":"<p>Please note that a cleaning policy is in place on scratch: all files older than 30 days will be deleted by a script that runs daily, so please ensure that you do not target this filesystem as a long term storage. Furthermore, kindly note that in order to avoid performance and stability issues on the scratch filesystem, if the occupancy grows above the critical limit of 60% we will be forced to ask you immediate action to remove unnecessary data: if the occupancy continues to grow and we reach 80%, we will then need to free up disk space manually removing files and folders without further notice.</p> <p>As a matter of fact, when the occupancy goes above 80% the Lustre filesystem shows a performance degradation that affects all users. The same applies with large numbers of small files, since the Lustre filesystem is not behaving ideally when dealing with high volumes of small files.</p> <p>Keep also in mind that data on scratch are not backed up, therefore users are advised to move valuable data to the /project filesystem or alternative storage facilities as soon as batch jobs are completed.</p> <p>Note</p> <p>Do not use the <code>touch</code> command to prevent the cleaning policy from removing files, because this behaviour would deprive the community of a shared resource.</p>"},{"location":"storage/filesystems/#users","title":"Users","text":"<p>Users are not supposed to run jobs from this filesystem because of the low performance. In fact the emphasis on the <code>/users</code> filesystem is reliability over performance: all home directories are backed up with GPFS snapshots and no cleaning policy is applied.</p>"},{"location":"storage/filesystems/#quota","title":"Quota","text":"<p>The $HOME environment variable points to the personal folder /users/: please, keep in mind that you cannot exceed the 50 GB - 500 K files quota enforced on $HOME. Expiration <p>Warning</p> <p>All data will be deleted 3 months after the closure of the user account without further warning.</p>"},{"location":"storage/filesystems/#store-on-capstore","title":"Store on Capstore","text":"<p>The <code>/capstor/store</code> mount point of the Lustre file system <code>capstor</code> is intended for high-performance per-project storage on Alps. The mount point is accessible from the User Access Nodes (UANs) of Alps vClusters.</p> <p>Note</p> <p>Capstore store is not yet mounted on Eiger.</p> <p>Info</p> <p><code>/capstor/store</code> is equivalent to the  <code>/project</code> and <code>/store</code> GPFS mounts on the old Daint system.</p> <p>The mount point features subfolders named after tenants and customers: on <code>daint.alps</code>, the only tenant currently available is CSCS, therefore all customer folders are located under the folder <code>/capstor/store/cscs</code> </p> <ul> <li>UserLab customers can access their project folder on daint.alps  with the <code>$PROJECT</code> environment variable, that targets the personal folder <code>/capstor/store/cscs/userlab/group_id/$USER</code>.</li> <li>Please note that users need to create their own sub-folders under the project folder, as they are not created automatically.</li> <li>Contractual partners will find their project folder under the corresponding contractual name listed in <code>/capstor/store/cscs</code>. For instance, EMPA users will have their projects listed under the <code>/capstor/store/cscs/empa</code> folder, and so on </li> </ul> <p>Data on <code>/capstor/store</code> is backed up with no cleaning policy: it provides intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different vClusters. The performance of the Lustre file system (read and write) increases using larger files, therefore you should consider to archive small files with the tar utility. Quota</p>"},{"location":"storage/filesystems/#quota_1","title":"Quota","text":"<p>Access to <code>/capstor/store</code> is granted to all users with a production or large development project upon request at the time of proposal submission: please note that applicants should justify the requested storage as well as they do for compute resources. Each group folder has a quota space allocated that allows maximum 1 M files and 150 TB of disk space at present. Expiration </p> <p>Warning</p> <p>All data will be deleted 3 months after the end of the project without further warning!</p>"},{"location":"storage/filesystems/#project","title":"Project","text":"<p>This is a shared - parallel file system based on the IBM GPFS software. It is accessible from the login nodes of all CSCS platforms using the native GPFS client through Infiniband or ethernet, however it is mounted read-only on the compute nodes of the Cray computing systems.</p> <p>Warning</p> <p>Users are not allowed to run jobs from this file system because of the low performance. </p> <p>The <code>$PROJECT</code> environment variable targets the personal folder <code>/project/&lt;group_id&gt;/$USER</code> on the GPFS: please note that users need to create their own sub-folders under the project folder, as they are not created automatically. Data is backed up with GPFS snapshots and no cleaning policy is applied: it provides intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different platforms. Read and write performance increase using larger files, therefore you should consider to archive small files with the tar utility.</p>"},{"location":"storage/filesystems/#quota_2","title":"Quota","text":"<p>Access to <code>/project</code> is granted to all users with a production or large development project upon request at the time of proposal submission: please note that applicants should justify the requested storage as well as they do for compute resources. Each group folder has a quota space allocated that allows maximum 50 K files per TB of disk space. Expiration</p> <p>Warning</p> <p>All data will be deleted 3 months after the end of the project without further warning.</p>"},{"location":"storage/filesystems/#store","title":"Store","text":"<p>Users are NOT supposed to run jobs from this filesystem because of the low performance. This is a shared - parallel filesystem based on the IBM GPFS software. It is accessible from the login nodes of all CSCS platforms using the native GPFS client through Infiniband or ethernet, however it is mounted read-only on the compute nodes of the Cray computing systems.</p> <p>Data is backed up with GPFS snapshots and no cleaning policy is applied: it provides long term storage for large amount of datasets, code or scripts that need to be accessed from different platforms. It is also intended for large files: performance increases when using larger files, therefore you should consider archiving small files with the tar utility.</p>"},{"location":"storage/filesystems/#quota_3","title":"Quota","text":"<p>Access to /store can only be bought signing a contract with CSCS. Data and inode quotas are group based: the quota is enforced according to the signed contract, with maximum 50 K files per TB of disk space. Expiration</p> <p>Warning</p> <p>All data will be deleted 3 months after the end of the contract.</p>"},{"location":"storage/longterm/","title":"Long Term Storage (LTS)","text":"<p>The Long Term Storage (LTS) service enables CSCS users to preserve their scientific data and ensures that it can be publicly accessed through a persistent identifier. The current implementation of the LTS service addresses the first two principles of the FAIR quadrant: findable and accessible.</p> <ul> <li> Findable Data and supplementary materials have sufficiently rich metadata anda  unique and persistant identifier.</li> <li> Accessible Metadata and data are understandable to humans and machines. Data is deposited in a trusted repository.</li> <li> Interoperable Metadata use a formal, accessible, shared, and broadly applicable language for knowledge representation.</li> <li> Reusable Data and collections have a clear usage license and provide accurate information on provenance.</li> </ul> <p>These are the main features of the service:</p> <ul> <li>Storage repository with long term retention capabilities (10 years);</li> <li>Provide persistent identifiers;</li> <li>Ability to set public access to data when needed;</li> <li>Data stored in LTS easily accessible from a web browser (HTTP protocol);</li> <li>RESTful API to integrate with third party applications/portals;</li> <li>Scalable service that can cope with large volumes of data;</li> <li>Resiliency due to data protection measures against hardware/software failures;</li> <li>Clear licensing of the data.</li> </ul>"},{"location":"storage/longterm/#service-description","title":"Service Description","text":"<p>The main unit of the LTS workflow is the data collection. A data collection is a group of data files enriched with a set of metadata attributes and a persistent ID referencing the entire collection. In other contexts such an entity might be called dataset, data aggregate or data block.</p> <p>The data files are store in the CSCS Object Store thus the data collection and its associated PID handle (the specific type of persistent ID used by LTS) will contain a list of URLs. There is no need to have special clients to access the data URLs or the PID handle, standard HTTP client like a browser or the <code>curl</code> command are sufficient.</p> <p>The PID handles use for the LTS service are the one provided by the CSCS PID service. This enables the LTS service to guarantee the consistency between data collection, object store data and PID handle.</p>"},{"location":"storage/longterm/#pricing","title":"Pricing","text":"<p>As of 2021:</p> <ul> <li>Users from the free User Lab program are entitled to use 2 TB of LTS storage quota (for 10 years) free of charge per project</li> <li>Currently additional space can be purchased for CHF 600.- for each Terabyte (for 10 years)</li> </ul>"},{"location":"storage/longterm/#prerequisites","title":"Prerequisites","text":"<p>In order to create collections and upload files into the LTS service, a user needs the following prerequisites:</p> <ul> <li>CSCS project with a quota on the LTS (and/or LTS-TDS) facility</li> <li>HTTP client: for example <code>curl</code> or Python requests.</li> <li>Keycloak registered client (only to access the service via RESTful API, not needed when using the web portal)</li> <li> <p>Outgoing connectivity to the following services:</p> Service URL Description LTS Prod https://lts.cscs.ch Production LTS service LTS TDS https://lts-tds.cscs.ch Test LTS service Keycloak https://auth.cscs.ch Authentication service Object Store https://object.cscs.ch Object Store service PID https://hdl.handle.net PID service </li> </ul> <p>In order to download files from the LTS service, a user needs a web browser or any other HTTP client like <code>curl</code>.</p>"},{"location":"storage/longterm/#creating-collections-and-uploading-files","title":"Creating collections and uploading files","text":"<p>LTS can be accessed in two ways:</p> <ul> <li>the web portal available at lts.cscs.ch;</li> <li>the LTS RESTful API, whose endpoints are described by the online documentation at lts.cscs.ch/api</li> </ul>"},{"location":"storage/longterm/#authentication","title":"Authentication","text":"<p>The LTS authenticates users based on the CSCS authentication service, therefore a user needs a valid CSCS account in order to access the service. The LTS web portal will guide the user through the authentication process; in order to access LTS through the RESTful API, the user will need to create a Keycloak token first.</p>"},{"location":"storage/longterm/#authorization","title":"Authorization","text":"<p>LTS data collections belong to a CSCS project: data can be stored in the LTS service after the principal investigator of the project has granted a project member the permissions to store data on behalf of the project. This is done by enabling the LTS facility for the user on the CSCS Account and Resources Management Tool.</p>"},{"location":"storage/longterm/#data-collection-creation-workflow","title":"Data collection creation Workflow","text":"<p>The typical LTS workflow will involve several steps, as described below. During the process, the data collection will go through the following states:</p> <pre><code>graph LR\n  A[NEW] --&gt; B[COMMITTED]\n  B --&gt; C[VALIDATED]\n  C --&gt; D[HANDLE ASSIGNED]\n  D --&gt; E[COMPLETED]</code></pre>"},{"location":"storage/longterm/#data-collection-definition","title":"Data collection definition","text":"<p>The workflow begins with the creation of a data collection. A minimum set of attributes is necessary to create the data collection; the most important ones are the following:</p> <ul> <li>name of the collection</li> <li>brief description</li> <li>project owning the data</li> <li>list of metadata attributes</li> <li>list of data files</li> <li>data files license</li> </ul> <p>The initial state of a data collection is <code>NEW</code>: the list of attributes and data files can be specified both at creation time or also added afterwards. The list of data files must contain the filename and the corresponding md5 checksum.</p> <p>Currently all the LTS data collections are considered containing public data: in future, the user will be able to specify whether the data collection is going to be public or private.</p> <p>If the data files are covered by copyright the user have to select an appropriate license. The default LTS license is CC BY 4.0.</p>"},{"location":"storage/longterm/#data-collection-inspection","title":"Data collection inspection","text":"<p>The collection can be inspected immediately after its creation. This is useful to check the attributes/objects already included in its definition, the object checksums, the overall state and the state of the single objects and their temporary URLs.</p>"},{"location":"storage/longterm/#data-collection-update-and-data-upload","title":"Data collection update and data upload","text":"<p>After the collection has been created, it can be modified during a transient phase: meanwhile, the collection state is <code>NEW</code> and the user can do the following:</p> <ul> <li>add additional data files</li> <li>add additional metadata attributes</li> <li>update already defined data files/attributes</li> <li>delete already defined data files/attributes</li> <li>upload the data files</li> </ul> <p>The LTS service generates a set of object store temporary URLs, one for each data file, which will be used to upload them to the object store. LTS is not involved in the upload operation, the data path comes from the user machine to the CSCS Object Store servers.</p>"},{"location":"storage/longterm/#licensing","title":"Licensing","text":"<p>When you prepare a data collection you must specify the license under which you publish your data. The default is the Creative Commons CC BY 4.0 license, but you can choose other ones. This Licensing guide provides information about the data licenses available in LTS.</p> <p>Todo</p> <p>the link to the licensing guide on Confluence KB was broken.</p>"},{"location":"storage/longterm/#commit","title":"Commit","text":"<p>At the end of the creation/update/upload phase, the user has to declare that the definition of the collection is complete. This is done by the user, who sends a collection commit request: after the commit request, the collection state changes from <code>NEW</code> to <code>COMMITTED</code> and from that point on the data collection definition cannot be modified anymore.</p>"},{"location":"storage/longterm/#validation","title":"Validation","text":"<p>Once the LTS service has received the commit request, it starts performing the validation operations needed to assess whether the files uploaded are consistent with the the checksums defined in the collection. If a mismatch is found, then the collection gets a <code>FAILED</code> status and the issue is reported back to the user. If all files are found in the object store and they have the correct checksum, the collection state is set to <code>VALIDATED</code>. At the end of the validation process, LTS sets the Object Store container ACLs in order to make the container world readable.</p>"},{"location":"storage/longterm/#handle-assignment","title":"Handle assignment","text":"<p>When the collection has entered the <code>VALIDATED</code> state, it's time for the LTS service to talk with the PID service and ask for a handle. The handle is attached to the collection and at that point the creation workflow of the collection is complete and the collection enter in the state <code>HANDLE_ASSIGNED</code> and the state COMPLETED shortly after that.</p>"},{"location":"storage/longterm/#dealing-with-failures","title":"Dealing with Failures","text":"<p>After the collection is committed by the user, LTS performs a serie of checks on the uploaded data and requests an handle to the ePIC handle service. If any error occurs during this phase the collection state will be <code>FAILED</code>. Possible reasons for this state are:</p> <ul> <li>a failure in one of the LTS microservices</li> <li>a failure in one of the underneath services (object store, handle server, database server etc ..)</li> <li>one or more data files were not uploaded</li> <li>one or more data file checksums are wrong</li> </ul> <p>When in state FAILED the collection is still editable. The user will have to review the content of the collection, fix the checksum or re-upload files if he/she spots anything wrong or missing. Then the collection needs to be saved and the validation retried. The validation process can be retried clicking on \"Retry Data Collection\" from the web portal or through the <code>commit</code> endpoint if using the RESTful API.</p>"},{"location":"storage/longterm/#downloading-data-from-lts","title":"Downloading data from LTS","text":"<p>The data is publicly accessible via the HTTP protocol once the status of the collection has entered the <code>COMPLETED</code> step. Its URL can be found under the \"Handle\" attribute of the data collection.</p>"},{"location":"storage/longterm/#further-documentation","title":"Further Documentation","text":"<p>Todo</p> <p>All of the links in this section (except the video) on the Confluence KB were broken.</p> <p>The Long Term Storage webinar, held in June 2021, provides an description of the Long Term Storage service use case, its architecture and workflow, and a demonstration of the Web Portal.</p> <p>The RESTful API HowTo provides some usage examples for the LTS API with curl and Python.</p> <p>The Web portal: create a data collection page provides usage examples for the creation of a data collection in the LTS web portal.</p> <p>The Web portal: define and upload objects page provides usage examples for definition and the upload of data collection objects in the LTS web portal.</p> <p>The Licensing guide provides information about the data licenses available in LTS.</p> <p>The Landing page page provides information about the LTS landing page.</p>"},{"location":"storage/object/","title":"Object Storage","text":"<p>Note</p> <p>This page is currently incomplete and it is being updated following recent developments.</p>"},{"location":"storage/object/#s3","title":"S3","text":"<p>CSCS offers a public cloud object storage service, based on the Ceph Object Gateway. The service can be accessed from S3-compatible clients.</p>"},{"location":"storage/object/#general-information","title":"General Information","text":"<ul> <li>Endpoint: https://rgw.cscs.ch</li> <li>URL: path-style in the format <code>https://rgw.cscs.ch/%(bucket)s/key-name</code></li> <li>Publicly accessible object links: <code>https://rgw.cscs.ch/&lt;tenant&gt;:&lt;bucket-name&gt;/key-name</code><ul> <li>after setting proper bucket policy</li> </ul> </li> </ul>"},{"location":"storage/object/#usage-examples","title":"Usage Examples","text":""},{"location":"storage/object/#aws-cli","title":"AWS CLI","text":""},{"location":"storage/object/#configuration","title":"Configuration","text":"<p>The first step is to configure the profile:</p> <pre><code>&gt; aws configure --profile naret-testuser\nAWS Access Key ID [None]: [REDACTED]\nAWS Secret Access Key [None]: [REDACTED]\nDefault region name [None]: cscs-zonegroup\nDefault output format [None]:\n</code></pre> <p>Then, settings such as the default endpoint and the path-style URLs can be placed in the configuration file:</p> <pre><code>[profile naret-testuser]\nendpoint_url = https://rgw.cscs.ch\nregion = cscs-zonegroup\ns3 =\n    addressing_style = path\n</code></pre>"},{"location":"storage/object/#creating-a-pre-signed-url","title":"Creating a pre-signed URL","text":"<pre><code>&gt; aws --profile=naret-testuser s3 presign s3://test-bucket/file.txt --expires-in 300\n\nhttps://rgw.cscs.ch/test-bucket/file.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=IA6AOCNMKPDXQ0YNA3DP%2F20241209%2Fcscs-zonegroup%2Fs3%2Faws4_request&amp;X-Amz-Date=20241209T080748Z&amp;X-Amz-Expires=300&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=f2e2adb457f6fd43401124e4ea2650fba528e614ab661f9c05e2fa2e77691b5d\n</code></pre> <p>Notice that the tenant part is missing from the URL: this is because S3 doesn't natively deal with multitenancy. The correct object is retrieved based on the access key. A more thorough explanation can be found in the RGW documentation.</p>"},{"location":"storage/object/#making-a-buckets-contents-anonymously-accessible-from-the-internet","title":"Making a bucket's contents anonymously accessible from the Internet","text":"<p>First, a bucket policy needs to be written:</p> <pre><code>&gt; cat test-public-bucket-anon-from-internet.json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": [\n        \"arn:aws:s3:::test-public-bucket/*\",\n        \"arn:aws:s3:::test-public-bucket\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>Then, it can be applied to the bucket:</p> <pre><code>&gt; aws --profile=naret-testuser s3api put-bucket-policy \\\n      --bucket test-public-bucket --policy \\\n      file://test-public-bucket-anon-from-internet.json\n</code></pre> <p>At this point, the objects in test-public-bucket are accessible via direct links:</p> <pre><code>&gt; s3cmd --configure\n\nEnter new values or accept defaults in brackets with Enter.\nRefer to user manual for detailed description of all options.\n\nAccess key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\nAccess Key: [REDACTED]\nSecret Key: [REDACTED]\nDefault Region [US]: cscs-zonegroup\n\nUse \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.\nS3 Endpoint [s3.amazonaws.com]: rgw.cscs.ch\n\nUse \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used\nif the target S3 system supports dns based buckets.\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: rgw.cscs.ch/%(bucket)s\n\nEncryption password is used to protect your files from reading\nby unauthorized persons while in transfer to S3\nEncryption password:\nPath to GPG program:\n\nWhen using secure HTTPS protocol all communication with Amazon S3\nservers is protected from 3rd party eavesdropping. This method is\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]: Yes\n\nOn some networks all internet access must go through a HTTP proxy.\nTry setting it here if you can't connect to S3 directly\nHTTP Proxy server name:\n\nNew settings:\n  Access Key: [REDACTED]\n  Secret Key: [REDACTED]\n  Default Region: cscs-zonegroup\n  S3 Endpoint: rgw.cscs.ch\n  DNS-style bucket+hostname:port template for accessing a bucket: rgw.cscs.ch/%(bucket)s\n  Encryption password:\n  Path to GPG program: None\n  Use HTTPS protocol: True\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n</code></pre> <p>And then confirm.</p> <p>IMPORTANT: The configuration is not complete yet.</p> <pre><code>&gt; s3cmd ls s3://test-bucket\nERROR: S3 error: 403 (SignatureDoesNotMatch)\n</code></pre> <p>To fix this, it is necessary to edit the <code>.s3cfg</code> file, normally located in the user's home directory, and change the <code>signature_v2</code> setting to true.</p> <pre><code>~ &gt; cat .s3cfg | grep signature_v2\nsignature_v2 = True\n\n&gt; s3cmd ls s3://test-bucket\n2024-12-09 08:05           15  s3://test-bucket/file.txt\n</code></pre>"},{"location":"storage/object/#cyberduck","title":"Cyberduck","text":""},{"location":"storage/object/#configuration_1","title":"Configuration","text":"<p>In order to be able to connect to the S3 endpoint using Cyberduck, a profile supporting path-style requests must be downloaded from here.</p> <p></p>"},{"location":"storage/transfer/","title":"Data Transfer","text":""},{"location":"storage/transfer/#external-transfer","title":"External Transfer","text":"<p>CSCS currently offers the CSCS Globus online endpoint for uploading and downloading data from and to CSCS:</p> <p>The recommended way to transfer data externally occurs via the CSCS globus-online endpoint.</p> <ol> <li>Follow the official get started documentation to login<ul> <li>in case you don't have an organisation account, you can just use the option \"Sign in with Google\" </li> </ul> </li> <li>Use the file manager to search for an endpoint typing \"CSCS\"<ul> <li>Please make sure that the login page belongs to the cscs.ch domain (shown in the URL)</li> <li>The CSCS endpoint requires authentication, therefore use your CSCS credentials to log in </li> </ul> </li> <li>Once logged in, you can trasfer data to and from CSCS.<ul> <li>if you want to transfer the data to another endpoint, just search for it and transfer the data</li> <li>if you want to download the data to your local system, you will need the Globus Connect Personal client: the client will turn your local system into an endpoint, so you will be able to select it and transfer the data.</li> </ul> </li> </ol> <p>For more information about Globus Connect Personal, please read the official Frequently Asked Questions.</p> <p>Currently Globus provide the following mount points at CSCS:</p> Mount Point Description <code>/scratch/snx3000</code> old Daint scratch area <code>/store</code> old Daint store area <code>/project</code> old Daint project area <code>/users</code> old Daint home directory <code>/scratch/shared</code> old Scratch-Shared area ( old meteoswiss clusters ) <code>/iopsstor/scratch/cscs</code> Mounted on Clariden <code>/capstor/scratch/cscs</code> New Alps Daint scratch area <code>/capstor/store/cscs</code> New Alps Daint store area <code>/capstor/users/cscs</code> Home directory for Bristen/Scopi/Errigal <code>/vast/users/cscs</code> New Alps vclustewrs home directory  ( Alps Daint and others )"},{"location":"storage/transfer/#internal-transfer","title":"Internal Transfer","text":"<p>The Slurm queue <code>xfer</code> is available on Piz Daint (Cray XC) and daint.alps to address data transfers between internal CSCS file systems. The queue has been created to transfer files and folders from /users, /project, /store or /capstor/store to the /capstor/scratch and /scratch file systems (stage-in) and vice versa (stage-out). Currently the following commands are available on the cluster supporting the queue xfer:</p> <pre><code>cp\nmv\nrm\nrsync\n</code></pre> <p>You can adjust the Slurm batch script below to transfer your input data on <code>$SCRATCH</code>, setting the variable command to the unix command that you intend to use, choosing from the list given above:</p> <pre><code>#!/bin/bash -l\n#\n#SBATCH --time=02:00:00\n#SBATCH --ntasks=1\n#SBATCH --partition=xfer\n\ncommand=\"rsync -av\"\necho -e \"$SLURM_JOB_NAME started on $(date):\\n $command $1 $2\\n\"\nsrun -n $SLURM_NTASKS $command $1 $2\necho -e \"$SLURM_JOB_NAME finished on $(date)\\n\"\n\nif [ -n \"$3\" ]; then\n  # unset memory constraint enabled on xfer partition\n  unset SLURM_MEM_PER_CPU\n  # submit job with dependency\n  sbatch --dependency=afterok:$SLURM_JOB_ID $3\nfi\n</code></pre> <p>The template Slurm batch script above requires at least two command line arguments, which are the source and the destination files (or folders) to be copied. The stage script may take as third command line argument the name of the production Slurm batch script to be submitted after the stage job: the Slurm dependency flag <code>--dependency=afterok:$SLURM_JOB_ID</code> ensures that the production job can begin execution only after the stage job has successfully executed (i.e. ran to completion with an exit code of zero).</p> <p>You can submit the stage job with a meaningful job name as below:</p> <pre><code># stage-in and production jobs\n$ sbatch --job-name=stage_in stage.sbatch \\\n         ${PROJECT}/&lt;source&gt; ${SCRATCH}/&lt;destination&gt; \\\n         production.sbatch\n</code></pre> <p>The Slurm flag <code>--job-name</code> will set the name of the stage job that will be printed in the Slurm output file: the latter is by default the file <code>slurm-${SLURM_JOB_ID}.out</code>, unless you set a specific name for output and error using the Slurm flags <code>-e/--error</code> and/or <code>-o/--output</code> (e.g.: <code>-o %j.out -e %j.err</code>, where the Slurm symbol <code>%j</code> will be replaced by <code>$SLURM_JOB_ID</code>). l The stage script will also submit the Slurm batch script production.sbatch given as third command line argument. The production script can submit in turn a stage job to transfer the results back. E.g.:</p> <pre><code># stage-out\nsbatch --dependency=afterok:${SLURM_JOB_ID} --job-name=stage_out \\\n       stage.sbatch ${SCRATCH}/&lt;source&gt; ${PROJECT}/&lt;destination&gt;\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p>Todo</p> <p>Documentation for tools used on the vClusters, including:</p> <ul> <li>slurm</li> <li>uenv</li> <li>container engine</li> <li>debuggers and profilers</li> </ul> <p>We document the tools and their interfaces here, but we don't put all the documentation for a tool here.</p> <ul> <li>e.g Documentation on how to build software using uenv, is in another section.</li> <li>e.g Documentation on how to use Podman to build containers, is in another section.</li> </ul>"},{"location":"tools/slurm/","title":"slurm","text":""},{"location":"tools/slurm/#slurm_1","title":"SLURM","text":"<p>CSCS uses the SLURM as its workload manager to efficiently schedule and manage jobs on Alps vClusters. SLURM is an open-source, highly scalable job scheduler that allocates computing resources, queues user jobs, and optimizes workload distribution across the cluster. It supports advanced scheduling policies, job dependencies, resource reservations, and accounting, making it well-suited for high-performance computing environments.</p>"},{"location":"tools/slurm/#accounting","title":"Accounting","text":"<p>Todo</p> <p>document <code>--account</code>, <code>--constrant</code> and other generic flags.</p>"},{"location":"tools/slurm/#partitions","title":"Partitions","text":"<p>At CSCS, SLURM is configured to accommodate the diverse range of node types available in our HPC clusters. These nodes vary in architecture, including CPU-only nodes and nodes equipped with different types of GPUs. Because of this heterogeneity, SLURM must be tailored to ensure efficient resource allocation, job scheduling, and workload management specific to each node type.</p> <p>Each type of node has different resource constraints and capabilities, which SLURM takes into account when scheduling jobs. For example, CPU-only nodes may have configurations optimized for multi-threaded CPU workloads, while GPU nodes require additional parameters to allocate GPU resources efficiently. SLURM ensures that user jobs request and receive the appropriate resources while preventing conflicts or inefficient utilization.</p> <p>The following sections will provide detailed guidance on how to use SLURM to request and manage CPU cores, memory, and GPUs in jobs. These instructions will help users optimize their workload execution and ensure efficient use of CSCS computing resources.</p> <p></p>"},{"location":"tools/slurm/#nvidia-gh200-gpu-nodes","title":"NVIDIA GH200 GPU Nodes","text":"<p>Todo</p> <p>document how slurm can be used on the Grace-Hopper nodes.</p> <p>Note how you can link to this section from elsewhere using the anchor above, e.g.:</p> <pre><code>[using slurm on Grace-Hopper][gh200-slurm]\n</code></pre> <p>Link to the Grace-Hopper overview.</p> <p>An example of using tabs to show srun and sbatch useage to get one GPU per MPI rank:</p> sbatchsrun <pre><code>#!/bin/bash\n#SBATCH --job-name=affinity-test\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\n#SBATCH --gpus-per-task=1\n\nsrun affinity\n</code></pre> <pre><code>&gt; srun -n8 -N2 --gpus-per-task=1 affinity\n</code></pre> <p></p>"},{"location":"tools/slurm/#amd-cpu","title":"AMD CPU","text":"<p>Todo</p> <p>document how slurm is configured on AMD CPU nodes (e.g. eiger)</p>"},{"location":"tools/uenv/","title":"uenv","text":"<p>Uenv are user environments that provide scientific applications, libraries and tools on Alps. This article will explain how to find, dowload and use uenv on the command line, and how to enable them in SLURM jobs.</p> <p>Uenv are typically application-specific, domain-specific or tool-specific - each uenv contains only what is required for the application or tools that it provides.</p> <p>Each uenv is packaged in a single file (in the Squashfs file format), that stores a compressed directory tree that contains all of the software, tools and other information like modules, required to provide a rich environment.</p> <p>Each environment contains a software stack, comprised of compilers, libraries, tools and scientific applications, built using Spack.</p> <p>Warning</p> <p>This documentation is for the new uenv2 implementation of uenv, that is not yet installed on Alps.</p>"},{"location":"tools/uenv/#getting-started","title":"Getting started","text":"<p>After logging into an Alps cluster, you can quickly check the availability of uenv with the following commands:</p> <pre><code>&gt; uenv status\nthere is no uenv loaded\n&gt; uenv --version\n7.0.0\n</code></pre> <p>installing uenv</p> <p>The command line tool can be installed from source, if you are working on a cluster that does not have uenv installed, or if you need to test a new version.</p> manually installing uenv in the terminal<pre><code>git clone https://github.com/eth-cscs/uenv2.git\ncd uenv2\n\n# run the installation script.\n# this will install uenv2 in $HOME/.local/$(uname -m)/\n./install-alps-local.sh\n\n# update bashrc\necho \"export PATH=\\$HOME/.local/\\$(uname -m)/bin:\\$PATH\" &gt;&gt; $HOME/.bashrc\necho \"unset -f uenv\" &gt;&gt; $HOME/.bashrc\n</code></pre> <p>Warning</p> <p>Before uenv can be used, you need to log out then back in again and type \"uenv --version\" to verify that uenv has been installed. The version should be <code>6.0.0-dev</code> if succesfull.</p>"},{"location":"tools/uenv/#naming-uenv","title":"Naming uenv","text":"<p>Uenv are referred to using labels, where a label has the following form <code>name/version:tag@system%uarch</code>, for example <code>prgenv-gnu/24.11:v2@todi%gh200</code>.</p>"},{"location":"tools/uenv/#name","title":"<code>name</code>","text":"<p>the name of the uenv. In this case <code>prgenv-gnu</code>.</p>"},{"location":"tools/uenv/#version","title":"<code>version</code>","text":"<p>The version of the uenv. The format of <code>version</code> depends on the specific uenv. Often they use the <code>yy.mm</code> format, though they may also use the version of the software being packaged. For example the <code>namd/3.0.1</code> uenv packages version 3.0.1 of the popular NAMD simulation tool.</p>"},{"location":"tools/uenv/#tag","title":"<code>tag</code>","text":"<p>Used to differentiate between releases of a versioned uenv. Some examples of tags include:</p> <ul> <li><code>rc1</code>, <code>rc2</code>: release candidates.</li> <li><code>v1</code>: a first release typically made after some release candidates.</li> <li><code>v2</code>: a second release, that might fix issues in the first release.</li> </ul>"},{"location":"tools/uenv/#system","title":"<code>system</code>","text":"<p>The name of the Alps cluster for which the uenv was built.</p>"},{"location":"tools/uenv/#uarch","title":"<code>uarch</code>","text":"<p>The node type (microarchitecture) that the uenv is built for:</p> uarch CPU GPU comment gh200 4 72-core NVIDIA Grace (<code>aarch64</code>) 4 NVIDIA H100 GPUs zen2 2 64-core AMD Rome (<code>zen2</code>) - used in Eiger a100 1 64-core AMD Milan (<code>zen3</code>) 4 NVIDIA A100 GPUs mi200 1 64-core AMD Milan (<code>zen3</code>) 4 AMD Mi250x GPUs zen3 2 64-core AMD Milan (<code>zen3</code>) - only in MCH system"},{"location":"tools/uenv/#using-labels","title":"using labels","text":"<p>The uenv command line has a flexible interface for describing</p> <pre><code># search for all uenv on the current system that have the name prgenv-gnu\nuenv image find prgenv-gnu\n\n# search for all uenv with version 24.11\nuenv image find /24.11\n\n# search for all uenv with tag v1\nuenv image find :v1\n\n# seach for a specific version\nuenv image find prgenv-gnu/24.11:v1\n</code></pre> <p>By default, the <code>uenv</code> filters results to only those </p> <pre><code># log into the eiger vCluster\nssh eiger\n\n# this command will search for all pgrenv-gnu uenv on _eiger_\nuenv image find prgenv-gnu\n\n# use @ to search on a specific system, e.g. on daint:\nuenv image find prgenv-gnu@daint\n\n# this can be used to search for all uenv on another system:\nuenv image find @daint\n\n# the '*' is a wildcard used meaining \"all systems\"\n# this will show all images on all systems\nuenv image find @'*'\n\n# search for all images on Alps that were built for gh200 nodes.\nuenv image find @'*'%gh200\n</code></pre> <p>Note</p> <p>The wild card <code>*</code> used for \"all systems\" must always be escaped in single quotes: <code>@'*'</code>.</p>"},{"location":"tools/uenv/#finding-uenv","title":"Finding uenv","text":"<p>Uenv for programming environments, tools and applications are provided by CSCS on each Alps system.</p> <p>Info</p> <p>The same uenv are not installed on every system. Instead uenv that are supported for the users of that platform are provided.</p> <p>The available uenv images are stored in a registry, that can be queried using the <code>uenv image find</code>  command:</p> uenv image find<pre><code>&gt; uenv image find\nuenv                       arch  system  id                size(MB)  date\ncp2k/2024.1:v1             zen2  eiger   2a56f1df31a4c196   2,693    2024-07-01\ncp2k/2024.2:v1             zen2  eiger   f83e95328d654c0f   2,739    2024-08-23\ncp2k/2024.3:v1             zen2  eiger   7c7369b64b5fabe5   2,740    2024-09-18\neditors/24.7:rc1           zen2  eiger   e5fb284962908eed   1,030    2024-07-18\neditors/24.7:v2            zen2  eiger   4f0f2770616135b1   1,062    2024-09-04\njulia/24.9:v1              zen2  eiger   0ff97a74dfcaa44e     539    2024-11-09\nlinalg/24.11:rc1           zen2  eiger   b69f4664bf0cd1c4     770    2024-11-20\nlinalg/24.11:v1            zen2  eiger   c11f6c85028abf5b     776    2024-12-03\nlinalg-complex/24.11:v1    zen2  eiger   846a04b4713d469b     792    2024-12-03\nlinaro-forge/24.0.2:v1     zen2  eiger   65734ce35494a5f5     313    2024-07-18\nlinaro-forge/24.1:v1       zen2  eiger   b65d7c85adfb317a     344    2024-11-27\nnetcdf-tools/2024:v1       zen2  eiger   e7e508c34cf40ccd   3,706    2024-11-14\nprgenv-gnu/24.11:rc4       zen2  eiger   811469b00f030493     570    2024-11-21\nprgenv-gnu/24.11:v1        zen2  eiger   0b6ab5fc4907bb38     572    2024-11-27\nprgenv-gnu/24.7:v1         zen2  eiger   7f68f4c8099de257     478    2024-07-01\nquantumespresso/v7.3.1:v1  zen2  eiger   61d1f21881a65578     864    2024-11-08\n</code></pre> <p>The output above shows that there are 12 uenv (<code>prgenv-gnu</code>, <code>namd</code> , <code>cp2k</code> and <code>arbor</code>).</p>"},{"location":"tools/uenv/#downloading-uenv","title":"Downloading uenv","text":"<p>To use a uenv, it first has to be pulled from the registry to local storage where you can access it. For example, to use the <code>prgenv-gnu</code> uenv, use the uenv image pull command:</p> uenv image pull<pre><code># The following commands have the same effect\n\n# method 1: pull using the name of the uenv\n&gt; uenv image pull prgenv-gnu/24.2:v1\n\n# method 2: pull using the id of the image\n&gt; uenv image pull 3ea1945046d884ee\n</code></pre> <p>Note</p> <p>In order to pull images, a local directory for storing the images must first be created, and you will receive an error message. To create a repo in the default location, use the following command:</p> uenv image repo<pre><code>&gt; uenv repo create\n</code></pre> <p>Some images can be large, over 10 GB, and it can take a while to download from the registry.</p> <p>To view all uenv that have been pulled, and are ready to use use the <code>uenv image ls</code> command:</p> listing downloaded uenv<pre><code>&gt; uenv image ls\nuenv/version:tag                        uarch date       id               size\nnetcdf-tools/2024:v1                    gh200 2024-04-04 499c886f2947538e 1.2GB\nlinaro-forge/23.1.2:latest              gh200 2024-04-10 ea67dbb33801c7c3 342MB\nicon-wcp/v1:v3                          gh200 2024-03-11 3e8f96370a4685a7 8.3GB\nicon-wcp/v1:latest                      gh200 2024-03-11 3e8f96370a4685a7 8.3GB\n</code></pre>"},{"location":"tools/uenv/#accessing-restricted-software","title":"Accessing restricted software","text":"<p>By default, uenv can be pulled by all users on a system, with no restrictions.</p> <p>Some uenv are not available to all users, for exampl the vasp  images are only available for users who have a VASP license, who are added to the vasp6  group once then have provided CSCS with a copy of their license.</p> <p>To be able to pull such images, you first need to configure the token for that specific software. This step only needs to be performed once - once set up you will only need to perform it again if the token is changed, or if you need to use a different token for another uenv.</p> using a token to access VASP<pre><code>todo: update from the recent docs\n</code></pre> <p>Note</p> <p>As of 15 Oct 2024 , the only restricted software is VASP.</p>"},{"location":"vclusters/clariden/","title":"clariden","text":""},{"location":"vclusters/clariden/#clariden","title":"Clariden","text":"<p>Todo</p> <p>Introduction</p> <p>This page is a cut and paste of some of Todi's old documentation, which we can turn into a template.</p>"},{"location":"vclusters/clariden/#cluster-details","title":"Cluster Details","text":"<p>Todo</p> <p>a standardised table with information about</p> <ul> <li>number and type of nodes</li> </ul> <p>and any special notes</p>"},{"location":"vclusters/clariden/#logging-into-clariden","title":"Logging into Clariden","text":"<p>Todo</p> <p>how to log in, i.e. <code>ssh clariden.cscs.ch</code> via <code>ela.cscs.ch</code></p> <p>provide the snippet to add to your <code>~/.ssh/config</code>, and link to where we document this (docs not currently available)</p>"},{"location":"vclusters/clariden/#software-and-services","title":"Software and services","text":"<p>Todo</p> <p>information about CSCS services/tools available</p> <ul> <li>container engine</li> <li>uenv</li> <li>CPE</li> <li>... etc</li> </ul>"},{"location":"vclusters/clariden/#running-jobs-on-clariden","title":"Running Jobs on Clariden","text":"<p>Clariden uses SLURM as the workload manager, which is used to launch and monitor distributed workloads, such as training runs.</p> <p>See detailed instructions on how to run jobs on the Grace-Hopper nodes.</p>"},{"location":"vclusters/clariden/#storage","title":"Storage","text":"<p>Todo</p> <p>describe the file systems that are attached, and where.</p> <p>This is where <code>$SCRATCH</code>, <code>$PROJECT</code> etc are defined for this cluster.</p> <p>Refer to the specific file systems that these map onto (capstor, iopstor, waldur), and link to the storage docs for these.</p> <p>Also discuss any specific storage policies. You might want to discuss storage policies for MLp one level up, in the MLp docs.</p> <ul> <li>attached storage and policies</li> </ul>"},{"location":"vclusters/clariden/#calendar-and-key-events","title":"Calendar and key events","text":"<p>The system is updated every Tuesday, between 9 am and 12 pm. ...</p> <p>Todo</p> <p>notifications</p> <p>a calendar widget would be useful, particularly if we can have a central calendar, and a way to filter events for specific instances</p>"},{"location":"vclusters/clariden/#change-log","title":"Change log","text":"<p>special text boxes for updates</p> <p>they can be opened and closed.</p> <p>2024-10-15 reservation <code>daint</code> available again</p> <p>The reservation daint  is available again exclusively for Daint users that need to run their benchmarks for submitting their proposals, additionally to the debug  partition and free nodes. Please add the Slurm option --reservation=daint to your batch script if you want to use it</p> 2024-10-07 New compute node image deployed <p>New compute node image deployed to fix the issue with GPU-aware MPI.</p> <p>Max job time limit is decreased from 12 hours to 6 hours</p> 2024-09-18 Daint users <p>In order to complete the preparatory work necessary to deliver Alps in production, as of September 18 2024 the vCluster Daint on Alps will no longer be accessible until further notice: the early access will still be granted on T\u00f6di using the Slurm reservation option <code>--reservation=daint</code></p>"},{"location":"vclusters/clariden/#known-issues","title":"Known issues","text":"<p>TODO list of know issues - include links to known issues page</p>"}]}